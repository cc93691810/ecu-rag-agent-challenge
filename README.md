## ğŸ—ï¸ Architecture & Design

This project implements a **Retrieval-Augmented Generation (RAG) agent** for technical support of ECU (Embedded Control Unit) product lines. The system intelligently routes user questions to the correct documentation sources, retrieves relevant technical snippets, and generates accurate answers using a local LLM (`llama3.1`). All interactions are automatically logged to **MLflow** for observability and iteration.

Ollama + llama3.1:8b (https://github.com/ollama/ollama/releases)

```bash
curl -L -o ollama_installer.exe https://ollama.com/download/OllamaSetup.exe
ollama --version
ollama pull llama3.1:8b
ollama list
curl http://localhost:11434/api/tags
```

### ğŸ”§ Core Components

| Component | Responsibility | Technology |
|---------|----------------|-----------|
| **Router** | Analyzes user questions and determines which ECU series (`700`, `800B`, `800P`) to query. Handles comparisons, general queries, and unknown cases. | Rule-based keyword matching with regex |
| **Retriever** | Fetches relevant technical documentation chunks from ChromaDB vector stores based on the routed series. Supports single-series, multi-series, and "all-series" retrieval. | `ChromaDB` + `HuggingFace/bge-small-en-v1.5 Embeddings` (for embedding only; no OpenAI API used in generation) |
| **Generator** | Synthesizes a natural language answer using retrieved context and the user question. Runs entirely **locally**. | `Ollama` + `llama3.1` (via LangChain) |
| **Orchestrator** | Coordinates the full pipeline: route â†’ retrieve â†’ generate. Wraps execution in an MLflow run for tracking. | Custom Python function (`run_ecu_agent_with_mlflow`) |
| **MLflow Tracker** | Logs every user interaction as an MLflow Run, capturing input, routing decision, retrieval stats, and output for debugging and analysis. | Local MLflow (`./mlruns`) |

### ğŸ”„ Data Flow

```mermaid
graph LR
    A[User Question] --> B{Router}
    B -->|Series Decision| C[Retriever]
    C -->|Relevant Docs| D[Generator]
    D --> E[Final Answer]
    F[Orchestrator] -->|Starts Run| G[MLflow]
    A --> F
    B --> F
    C --> F
    D --> F
    E --> F
    F -->|Logs Params/Metrics| G
    G --> H[(./mlruns)]
```

### ğŸš€ FastAPI REST API

The RAG agent is exposed as a production-ready REST API using **FastAPI**, enabling easy integration with web applications, chatbots, or internal tools.

#### â–¶ï¸ Run api
```bash
cd ecu_agent/src
uvicorn api:app --host 0.0.0.0 --port 8000 --reload
```
#### â–¶ï¸ Test api
```PowerShell
$response = Invoke-RestMethod -Uri "http://127.0.0.1:8000/ask" `
    -Method Post `
    -Headers @{
        "accept" = "application/json"
        "Content-Type" = "application/json"
    } `
    -Body (@{
        question = "How much RAM does the ECU-850 have?"
    } | ConvertTo-Json)

$response
```

#### MLflow model for learning and test
Project Structure
```bash
ecu_agent/
â”‚
â”œâ”€â”€ data/                              # æ•°æ®æ–‡ä»¶
â”‚   â”œâ”€â”€ ECU-700_Series_Manual.md      # ECU-700ç³»åˆ—æ‰‹å†Œ
â”‚   â”œâ”€â”€ ECU-800_Series_Base.md        # ECU-800åŸºç¡€ç‰ˆæ‰‹å†Œ
â”‚   â””â”€â”€ ECU-800_Series_Plus.md        # ECU-800å¢å¼ºç‰ˆæ‰‹å†Œ
â”‚
â”œâ”€â”€ scripts/                           # è„šæœ¬æ–‡ä»¶
â”‚   â””â”€â”€ build_model.py                # æ„å»ºMLflowæ¨¡å‹
â”‚
â”œâ”€â”€ src/                              # æºä»£ç 
â”‚   â”œâ”€â”€ _ _init_ _.py                   # PythonåŒ…åˆå§‹åŒ–
â”‚   â”œâ”€â”€ model.py                      # MLflowæ¨¡å‹å®ç°
â”‚   â”œâ”€â”€ agent.py                      # æ™ºèƒ½ä½“é€»è¾‘
â”‚   â”œâ”€â”€ rag.py                        # RAGæ ¸å¿ƒåŠŸèƒ½
â”‚   â”œâ”€â”€ config.py                     # é…ç½®ç®¡ç†
â”‚   â”œâ”€â”€ utils.py                      # chunkingæ¨¡å—
â”‚   â””â”€â”€ tools.py                      # å·¥å…·æ¨¡å—
â”‚
â”œâ”€â”€ chroma_db/                        # å‘é‡æ•°æ®åº“å­˜å‚¨
â”‚   â”œâ”€â”€ ecu_700/                      # ECU-700å‘é‡æ•°æ®
â”‚   â”‚   â”œâ”€â”€ chroma.sqlite3
â”‚   â”‚   â”œâ”€â”€ index/
â”‚   â”‚   â””â”€â”€ ...
â”‚   â”œâ”€â”€ ecu_800B/                     # ECU-800åŸºç¡€ç‰ˆå‘é‡æ•°æ®
â”‚   â””â”€â”€ ecu_800P/                     # ECU-800å¢å¼ºç‰ˆå‘é‡æ•°æ®
â”‚
â”œâ”€â”€ models/                           # æ¨¡å‹æ–‡ä»¶
â”‚   â”œâ”€â”€ bge-small-en-v1.5/            # åµŒå…¥æ¨¡å‹
â”‚   â”‚   â”œâ”€â”€ 1_Pooling/
â”‚   â”‚   â”œâ”€â”€ config.json
â”‚   â”‚   â”œâ”€â”€ pytorch_model.bin
â”‚   â”‚   â””â”€â”€ ...
â”‚   â”‚
â”‚   â””â”€â”€ test_model/                   # MLflowæ‰“åŒ…çš„æ¨¡å‹
â”‚       â”œâ”€â”€ MLmodel                   # æ¨¡å‹é…ç½®æ–‡ä»¶
â”‚       â”œâ”€â”€ conda.yaml               # Condaç¯å¢ƒé…ç½®
â”‚       â”œâ”€â”€ python_env.yaml          # Pythonç¯å¢ƒé…ç½®
â”‚       â”œâ”€â”€ requirements.txt         # ä¾èµ–åˆ—è¡¨
â”‚       â”œâ”€â”€ artifacts/               # æ¨¡å‹å·¥ä»¶
â”‚       â”‚    â””â”€â”€ chroma_root -> ../../chroma_db  # ç¬¦å·é“¾æ¥æˆ–å¼•ç”¨
â”‚       â””â”€â”€code/src
â”‚
â”œâ”€â”€ pyproject.toml                  # Pythonä¾èµ–
â”œâ”€â”€ conda.yaml                      # Condaç¯å¢ƒé…ç½®
â”œâ”€â”€ README.md                       # é¡¹ç›®è¯´æ˜æ–‡æ¡£
â”œâ”€â”€ runMLflowModelServe.bat         # å¯åŠ¨mlflow test_modelè„šæœ¬
â”œâ”€â”€ dockfile                        # dockerfile
â”œâ”€â”€ .gitignore                      # Gitå¿½ç•¥æ–‡ä»¶
â”œâ”€â”€ .pylintrc                       # Gitå¿½ç•¥æ–‡ä»¶
â””â”€â”€ .env.example                    # ç¯å¢ƒå˜é‡ç¤ºä¾‹
```

### Docker in China
    "registry-mirrors": [
        "https://docker.xuanyuan.me"
    ]

### Future improvement
1.Try some other LLM instead of my loacl llama3.1:8b

2.Try some other chunking model

3.Learn more about the MLflow and implement on a real project

4.Regarding docker, test more.