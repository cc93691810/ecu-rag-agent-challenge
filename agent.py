from typing import TypedDict, List, Literal
from langchain_core.documents import Document
from langchain_ollama import ChatOllama
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langgraph.graph import StateGraph, END
import mlflow
import time
from rag import get_vectorstore

# è®¾ç½®æœ¬åœ° MLflow è·Ÿè¸ª URIï¼ˆé»˜è®¤å°±æ˜¯ ./mlrunsï¼Œå¯çœç•¥ï¼‰
mlflow.set_tracking_uri("./mlruns")  # å¯é€‰ï¼Œæ˜¾å¼æŒ‡å®š
mlflow.set_experiment("ECU-mlflow-test")

# ======================
# 1. å®šä¹‰ Agent çŠ¶æ€
# ======================
class ECUAgentState(TypedDict):
    """ECU Agent çš„å…¨å±€çŠ¶æ€"""
    user_question: str                          # ç”¨æˆ·è¾“å…¥çš„é—®é¢˜
    series_to_query: Literal["700", "800b", "800p", "unknown"]  # è·¯ç”±å†³ç­–
    retrieved_docs: List[Document]              # æ£€ç´¢åˆ°çš„æ–‡æ¡£
    final_answer: str                           # æœ€ç»ˆå›ç­”

# ======================
# 2. åˆå§‹åŒ– LLMï¼ˆå…¨å±€å¤ç”¨ï¼‰
# ======================
llm = ChatOllama(
    model="llama3.1:8b",
    base_url="http://localhost:11434",
    temperature=0.0,
    num_predict=256,
    timeout=120
)

# ======================
# 3. å®šä¹‰èŠ‚ç‚¹å‡½æ•°
# ======================
def route_question(state: ECUAgentState) -> dict:
    """æ ¹æ®é—®é¢˜å†…å®¹å†³å®šæŸ¥è¯¢å“ªä¸ª ECU ç³»åˆ—"""
    q = state["user_question"].lower()
    user_question = state["user_question"]  # ä¿æŒåŸå§‹å¤§å°å†™ç”¨äºæ˜¾ç¤º
    print(f"ğŸ” Analyzing question: '{user_question}'")

    # æ£€æµ‹é—®é¢˜ä¸­æ¶‰åŠçš„å‹å·
    has_700 = any(kw in q for kw in ["700", "750", "legacy"])
    has_800P = any(kw in q for kw in ["800b", "800 p", "plus", "800 plus", "850b", "ECU-850b"])
    has_800B = any(kw in q for kw in ["800 ", "base", "800 base", "850", "ECU-850"])
    # ç»Ÿè®¡åŒ¹é…çš„ç³»åˆ—æ•°
    series_count = sum([has_700, has_800B, has_800P])

    # æ£€æµ‹æ¯”è¾ƒæ„å›¾çš„å…³é”®å­—
    comparison_keywords = [
        "compare", "comparison", "difference", "vs", "versus", "between", 
        "vs.", "comparing", "contrast", "differences", "vs ", " versus ", "and"
    ]
    is_comparison = any(keyword in q for keyword in comparison_keywords)
    # æ£€æµ‹é€šç”¨æŸ¥è¯¢ï¼ˆæ¶‰åŠå¤šä¸ªå‹å·ï¼‰
    general_keywords = [
        "which ", "all ", "models", "How many", "est "
    ]
    is_general = any(keyword in q for keyword in general_keywords)

    # å¤„ç†æ¯”è¾ƒé—®é¢˜
    if is_comparison and series_count >= 2:
        # ç¡®å®šæ¶‰åŠå“ªäº›ç³»åˆ—
        multi_series = []
        if has_700:
            multi_series.append("700")
        if has_800B:
            multi_series.append("800B")
        if has_800P:
            multi_series.append("800P")
        result = f"multi:{','.join(multi_series)}"
        print(f"ğŸ¯ Route: '{user_question}' -> series_to_query: '{result}' (comparison detected)")
        return {"series_to_query": result}

    # å¤„ç†é€šç”¨æŸ¥è¯¢ï¼ˆéœ€è¦è·¨å¤šä¸ªç³»åˆ—æŸ¥æ‰¾ï¼‰
    if is_general:
        # æ£€æŸ¥é—®é¢˜ä¸­æ˜¯å¦æ˜ç¡®æŒ‡å®šäº†å‹å·èŒƒå›´
        if series_count > 0:
            multi_series = []
            if has_700:
                multi_series.append("700")
            if has_800B:
                multi_series.append("800B")
            if has_800P:
                multi_series.append("800P")
            if len(multi_series) > 1:
                result = f"multi:{','.join(multi_series)}"
                print(f"ğŸ¯ Route: '{user_question}' -> series_to_query: '{result}' (general query across series)")
                return {"series_to_query": result}
            if len(multi_series) == 1:
                result = multi_series[0]
                matched_kw = [kw for kw in ["700", "850 ", "850b", "750"] if kw in q.lower()][0]
                print(f"ğŸ¯ Route: '{user_question}' -> series_to_query: '{result}' (matched: {matched_kw})")
                return {"series_to_query": result}
        else:
            # é€šç”¨æŸ¥è¯¢ä½†æœªæŒ‡å®šå…·ä½“å‹å·ï¼ŒæŸ¥è¯¢æ‰€æœ‰ç³»åˆ—
            result = "multi:700,800B,800P"
            print(f"ğŸ¯ Route: '{user_question}' -> series_to_query: '{result}' (general query across ALL series)")
            return {"series_to_query": result}

    # å•ä¸€ç³»åˆ—æŸ¥è¯¢
    if has_700:
        result = "700"
        matched_kw = [kw for kw in ["700", "750", "legacy"] if kw in q][0]
        print(f"ğŸ¯ Route: '{user_question}' -> series_to_query: '{result}' (matched: {matched_kw})")
        return {"series_to_query": result}
    if has_800B:
        result = "800B"
        # æ‰¾åˆ°åŒ¹é…çš„å…³é”®è¯
        matched_kws = [kw for kw in ["800 ", "base", "800 base", "850", "ecu-850 "] if kw in q]
        matched_kw = matched_kws[0] if matched_kws else "850"
        print(f"ğŸ¯ Route: '{user_question}' -> series_to_query: '{result}' (matched: {matched_kw})")
        return {"series_to_query": result}
    if has_800P:
        result = "800P"
        matched_kws = [kw for kw in ["800p", "800 p", "plus", "800 plus", "850b", "ecu-850b "] if kw in q]
        matched_kw = matched_kws[0] if matched_kws else "850b"
        print(f"ğŸ¯ Route: '{user_question}' -> series_to_query: '{result}' (matched: {matched_kw})")
        return {"series_to_query": result}
    else:
        result = "unknown"
        print(f"ğŸ¯ Route: '{user_question}' -> series_to_query: '{result}' (no match found)")
        return {"series_to_query": result}

def retrieve_documents(state: ECUAgentState) -> dict:
    """ä»æœ¬åœ° ChromaDB æ£€ç´¢ç›¸å…³æ–‡æ¡£"""
    series = state["series_to_query"]
    all_docs=[]
    if series == "unknown":
        # ä»æ‰€æœ‰ç³»åˆ—ä¸­æ£€ç´¢ï¼ˆç”¨äºé€šç”¨æŸ¥è¯¢ï¼‰
        print("ğŸ”„ Unknown series detected - retrieving from ALL series")
        try:
            for s in ["700", "800B", "800P"]:
                print(f"  â¤ Retrieving from series {s}...")
                vectorstore = get_vectorstore(s)
                docs = vectorstore.similarity_search(
                    state["user_question"],
                    k=2  # ä»æ¯ä¸ªç³»åˆ—å–2ä¸ªæœ€ç›¸å…³çš„ chunks
                )
                # æ·»åŠ ç³»åˆ—æ ‡ç­¾åˆ°å…ƒæ•°æ®ï¼Œä¾¿äºåç»­åŒºåˆ†
                for doc in docs:
                    doc.metadata["series"] = s
                all_docs.extend(docs)
                print(f"    Retrieved {len(docs)} docs from series {s}")
        except Exception as e:
            print(f"âš ï¸ Universal retrieval failed: {e}")
            return {"retrieved_docs": []}

    # æ£€æŸ¥æ˜¯å¦ä¸ºå¤šç³»åˆ—æŸ¥è¯¢æ ¼å¼
    elif series.startswith("multi:"):
        # è§£æå¤šä¸ªç³»åˆ—
        series_list = series[6:].split(",")  # "multi:800B,800P" -> ["800B", "800P"]
        print(f"ğŸ”„ Multi-series retrieval: {series_list}")

        try:
            for s in series_list:
                print(f"  â¤ multi: Retrieving from series {s}...")
                vectorstore = get_vectorstore(s)
                docs = vectorstore.similarity_search(
                    state["user_question"],
                    k=2  # æ¯ä¸ªç³»åˆ—å– 3 ä¸ªæœ€ç›¸å…³çš„ chunks
                )
                # æ·»åŠ ç³»åˆ—æ ‡ç­¾åˆ°å…ƒæ•°æ®ï¼Œä¾¿äºåç»­åŒºåˆ†
                for doc in docs:
                    doc.metadata["series"] = s
                all_docs.extend(docs)
                print(f" Retrieved {len(docs)} docs from series {s}")
        except Exception as e:
            print(f"âš ï¸ Multi-retrieval failed: {e}")
            return {"retrieved_docs": []}

    else:
        # å•ä¸€ç³»åˆ—æŸ¥è¯¢ï¼ˆåŸé€»è¾‘ï¼‰
        try:
            vectorstore = get_vectorstore(series)
            docs = vectorstore.similarity_search(
                state["user_question"],
                k=2
            )
            # ä¸ºå•ä¸€ç³»åˆ—ä¹Ÿæ·»åŠ ç³»åˆ—æ ‡ç­¾
            for doc in docs:
                doc.metadata["series"] = series
            all_docs = docs
        except Exception as e:
            print(f"âš ï¸ Single-retrieval failed for series {series}: {e}")
            return {"retrieved_docs": []}

    print(f"âœ… Retrieved {len(all_docs)} documents from {len(set(d.metadata.get('series') for d in all_docs))} series")
    return {"retrieved_docs": all_docs}

def generate_answer(state: ECUAgentState) -> dict:
    """åŸºäºæ£€ç´¢ç»“æœç”Ÿæˆæœ€ç»ˆå›ç­”"""
    question = state["user_question"]
    docs = state["retrieved_docs"]

    if not docs:
        answer = "I don't have technical information about this ECU model."
    else:
        context = docs[0].page_content

        prompt = ChatPromptTemplate.from_template(
            """You are an expert automotive engineer assistant.
            Answer the question based ONLY on the following context.
            Do not make up information. If unsure, say "I don't know".

            Context:
            {context}

            Question: {question}
            Answer:"""
        )

        chain = prompt | llm | StrOutputParser()
        answer = chain.invoke({"context": context, "question": question})

    return {"final_answer": answer}

# ======================
# 4. æ„å»ºå¹¶è¿”å› LangGraph Agent
# ======================
def build_ecu_agent():
    """
    æ„å»ºå¹¶è¿”å›ä¸€ä¸ªå¯æ‰§è¡Œçš„ LangGraph ECU æŠ€æœ¯é—®ç­” Agentã€‚
    
    è¿”å›:
        Runnable: å¯é€šè¿‡ .invoke({"user_question": "..."}) è°ƒç”¨çš„ Agent
    """
    workflow = StateGraph(ECUAgentState)

    # æ·»åŠ èŠ‚ç‚¹
    workflow.add_node("route", route_question)
    workflow.add_node("retrieve", retrieve_documents)
    workflow.add_node("generate", generate_answer)

    # è®¾ç½®å…¥å£å’Œè¾¹
    workflow.set_entry_point("route")
    workflow.add_edge("route", "retrieve")
    workflow.add_edge("retrieve", "generate")
    workflow.add_edge("generate", END)

    # ç¼–è¯‘ä¸ºå¯æ‰§è¡Œåº”ç”¨
    app = workflow.compile()
    return app

# ä¾¿æ·å›å¤
def query_ecu_agent(question: str) -> str:
    """ä¾¿æ·å‡½æ•°ï¼šè¾“å…¥é—®é¢˜ï¼Œè¿”å›ç­”æ¡ˆ"""
    app = build_ecu_agent()
    result = app.invoke({
        "user_question": question,
        "series_to_query": "unknown",
        "retrieved_docs": [],
        "final_answer": ""
    })
    return result["final_answer"]

# ----------------------------
# 5. Orchestrator (with MLflow)
# ----------------------------
def run_ecu_agent_with_mlflow(user_question: str) -> dict:
    """å®Œæ•´æ‰§è¡Œæµç¨‹ + MLflow æ—¥å¿—"""

    # å®šä¹‰é»˜è®¤è¿”å›å€¼
    final_state = {
        "user_question": user_question,
        "series_to_query": "unknown",
        "retrieved_docs": [],
        "final_answer": "å¤„ç†ä¸­...",
        "success": False,
        "error": None
    }

    try:
        with mlflow.start_run(run_name=f"Q: {user_question[:40]}..."):
            # è®°å½•å¼€å§‹æ—¶é—´
            start_time = time.time()

            # è®°å½•å‚æ•°
            mlflow.log_param("question", user_question)
            mlflow.log_param("question_length", len(user_question))

            # æ„å»ºä»£ç†
            app = build_ecu_agent()

            # å‡†å¤‡åˆå§‹çŠ¶æ€
            initial_state = {
                "user_question": user_question,
                "series_to_query": "unknown",
                "retrieved_docs": [],
                "final_answer": ""
            }

            # æ‰§è¡Œä»£ç†
            agent_result = app.invoke(initial_state)  # ä½¿ç”¨ä¸åŒçš„å˜é‡å

            # æ›´æ–° final_state
            final_state.update(agent_result)
            final_state["success"] = True

            # è®¡ç®—æ‰§è¡Œæ—¶é—´
            execution_time = time.time() - start_time
            final_state["execution_time"] = execution_time

            # è®°å½•æŒ‡æ ‡
            mlflow.log_metric("execution_time", execution_time)
            mlflow.log_metric("docs_retrieved", len(final_state.get("retrieved_docs", [])))
            mlflow.log_metric("answer_length", len(final_state.get("final_answer", "")))

            # è®°å½•æ¨¡å‹è¾“å…¥è¾“å‡º
            mlflow.log_dict({
                "input": initial_state,
                "output": final_state
            }, "input_output.json")

            # è®°å½•æœ€ç»ˆçŠ¶æ€
            mlflow.log_dict(final_state, "final_state.json")

    except Exception as error:
        # é”™è¯¯å¤„ç†
        final_state.update({
            "success": False,
            "error": str(error),
            "final_answer": f"æŠ±æ­‰ï¼Œå¤„ç†é—®é¢˜æ—¶å‡ºé”™: {error}"
        })

        # è®°å½•é”™è¯¯
        if 'mlflow' in locals():
            mlflow.log_param("error", str(error))
            mlflow.log_metric("error_occurred", 1)

    # ç¡®ä¿æ€»æ˜¯è¿”å› final_state
    return final_state

###################################################
# def build_ecu_agent_HF():
#     # ä½¿ç”¨ flan-t5-smallï¼ˆæ›´å¿«ï¼Œé€‚åˆæµ‹è¯•ï¼‰
#     generator = pipeline(
#         "text2text-generation",
#         model="google/flan-t5-small",
#         max_new_tokens=200,
#         do_sample=False,
#         device=0 if torch.cuda.is_available() else -1
#     )
#     llm = HuggingFacePipeline(pipeline=generator)
#     tools = [query_ecu_700_series, query_ecu_800_series]
#     return create_react_agent(llm, tools)

# def build_ecu_agent_llama():
#     # ä½¿ç”¨æœ¬åœ° Ollama æœåŠ¡è°ƒç”¨ llama3.1:8b
#     llm = ChatOllama(
#         model="llama3.1:8b",          # å¿…é¡»ä¸ ollama list ä¸­çš„åç§°ä¸€è‡´
#         base_url="http://localhost:11434",  # Ollama é»˜è®¤åœ°å€
#         temperature=0.0,              # é™ä½éšæœºæ€§ï¼Œé€‚åˆé—®ç­”
#         timeout=120,                  # é˜²æ­¢é•¿å“åº”è¶…æ—¶
#         num_predict=256,               # æœ€å¤§ç”Ÿæˆé•¿åº¦
#     )
#     tools = [query_ecu_700_series, query_ecu_800_series]
#     return langchain.agents.create_react_agent(llm,tools)
